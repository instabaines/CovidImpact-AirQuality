{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "import fire\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "# to view all columns\n",
    "pd.set_option(\"display.max.columns\", None)\n",
    "\n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from tweepy import API\n",
    "from tweepy import Cursor\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import sys\n",
    "\n",
    "\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets():\n",
    "    \n",
    "    # class variables + tweepy authentication\n",
    "    def __init__(self, intervention_date = '2020-04-01', earliest_date = '2020-01-01', post_date = '2020-07-01'):\n",
    "        self.consumer_key = os.environ.get('consumer_key')\n",
    "        self.consumer_secret = os.environ.get('consumer_secret')\n",
    "        self.access_token = os.environ.get('access_token')\n",
    "        self.access_token_secret = os.environ.get('access_token_secret')\n",
    "        \n",
    "        self.auth = OAuthHandler(self.consumer_key, self.consumer_secret)\n",
    "        self.auth.set_access_token(self.access_token, self.access_token_secret)\n",
    "        self.auth_api = API(self.auth, wait_on_rate_limit=True)\n",
    "        \n",
    "        self.intervention_date = intervention_date\n",
    "        self.post_date = post_date\n",
    "        self.earliest_date = earliest_date\n",
    "        \n",
    "#     # country id \n",
    "#     def get_placeid(self, country):\n",
    "#         places = self.auth_api.geo_search(query=country, granularity=\"country\")\n",
    "#         place_id = places[0].id\n",
    "#         return place_id\n",
    "    \n",
    "    # get tweets\n",
    "    def get_tweets(self, hashtag, geocode):\n",
    "        pre_intervention_tweets = tweepy.Cursor(self.auth_api.search , q=hashtag, geocode = geocode, since_id = self.earliest_date).items(300)\n",
    "        post_intervention_tweets = tweepy.Cursor(self.auth_api.search , q=hashtag, geocode = geocode, since_id = self.post_date).items(300)\n",
    "        during_intervention_tweets = tweepy.Cursor(self.auth_api.search , q=hashtag, geocode = geocode, since_id = self.intervention_date).items(300)\n",
    "        \n",
    "        return pre_intervention_tweets, during_intervention_tweets, post_intervention_tweets\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def get_tweets(self, place_id):\n",
    "#         # tweets before covid intervention\n",
    "# #         pre_intervention_tweets = tweepy.Cursor(self.auth_api.search , q=self.keywords and (\"place:%s\" % place_id),  since_id = self.earliest_date).items(10000)\n",
    "#         tweets = tweepy.Cursor(self.auth_api.search , q = self.keywords and (\"place:%s\" % place_id),  since_id = self.start_date).items(100)\n",
    "#         pre_intervention_tweets = []\n",
    "#         for tweet in tweets:\n",
    "#             if tweet.created_at < self.end_date and tweet.created_at > self.start_date:\n",
    "#                 pre_intervention_tweets.append(tweet)        \n",
    "# #         tweets_ = tweepy.Cursor(self.auth_api.search , q = self.keywords and (\"place:%s\" % place_id),  since_id = self.start_date).items(100)\n",
    "# #         during_intervention_tweets = []\n",
    "# #         for tweet in tweets_:\n",
    "# #             if tweet.created_at < endDate and tweet.created_at > startDate:\n",
    "# #                 during_intervention_tweets.append(tweet)          \n",
    "# #       tweets post covid intervention \n",
    "#         post_intervention_tweets = tweepy.Cursor(self.auth_api.search , q=self.keywords and (\"place:%s\" % place_id),  since_id = self.intervention_date).items(1000)\n",
    "#         return pre_intervention_tweets, post_intervention_tweets\n",
    "    \n",
    "    # get tweet attributes ~ save to csv\n",
    "    def get_var(self, country, tweets, tag, period = 'pre_covid'):\n",
    "\n",
    "        alltweets = [] \n",
    "        alltweets.extend(tweets)\n",
    "\n",
    "        outtweets = [[tweet.id_str, tweet.user.screen_name, tweet.created_at, tweet.text, tweet.favorite_count, tweet.retweet_count, tweet.entities, tweet.lang] for tweet in alltweets]\n",
    "\n",
    "        #write to csv  \n",
    "        with open(f'{country}_{period}_{tag}_tweets.csv', 'w') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"id\", \"user\", \"created_at\",\"text\", \"favorites\", \"retweets\", \"entities\", \"language\"])\n",
    "            writer.writerows(outtweets)\n",
    "\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KE': '0.0236,37.9062,530km',\n",
       " 'NG': '9.0820,8.6753, 750km',\n",
       " 'ZA': '30.5595, 22.9375, 1000km',\n",
       " 'UG': '1.3733, 32.2903, 400km'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate class\n",
    "t = Tweets()\n",
    "\n",
    "# geocodes\n",
    "codes = {'KE' : \"0.0236,37.9062,530km\", 'NG' : \"9.0820,8.6753, 750km\", \n",
    "         'ZA' : \"30.5595, 22.9375, 1000km\", 'UG': \"1.3733, 32.2903, 400km\"}\n",
    "\n",
    "kenya = \"0.0236,37.9062,530km\"\n",
    "nigeria = \"9.0820,8.6753, 750km\"\n",
    "south_africa = \"30.5595, 22.9375, 1000km\"\n",
    "uganda = \"1.3733, 32.2903, 400km\"\n",
    "\n",
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = ['AirPollution', 'Environment', 'Ozone Layer', 'Global Warming', 'Climate Change', 'Greenhouse Gases', 'Trees', 'Carbon',\n",
    "        'Aerosals', 'Air', 'Save the planet', 'Factories', 'Hygroscopicity', 'Inversion', 'Sulfur', 'AIRS', 'ecosystem', 'Hydrochlorofluorocarbon',\n",
    "        'hydrocarbon', 'TAC', 'zero', 'pollutant', '#air', '#pollution', '#airpollution', '#coal', '#particles', '#smog', '#cleanair',\n",
    "       '#airqualityindex', '#climatechange', '#airquality', '#globalwarming', '#airpollutionawareness', '#airpollutioncontrol',\n",
    "       '#CleanEnergy', '#saveearth']\n",
    "len(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change country code to your country's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweets\n",
    "pre_bowl = []\n",
    "during_bowl = []\n",
    "post_bowl = []\n",
    "\n",
    "for hashtag in tags:\n",
    "    KE_pre_intervention_tweets, KE_during_intervention_tweets, KE_post_intervention_tweets = t.get_tweets(hashtag, codes['KE'])\n",
    "    pre_bowl.append(KE_pre_intervention_tweets)\n",
    "    during_bowl.append(KE_during_intervention_tweets)\n",
    "    post_bowl.append(KE_post_intervention_tweets)\n",
    "    \n",
    "# NG_pre_intervention_tweets, NG_post_intervention_tweets = get_tweets(ids['NG'])\n",
    "# UG_pre_intervention_tweets, UG_post_intervention_tweets = get_tweets(ids['UG'])\n",
    "# ZA_pre_intervention_tweets, ZA_post_intervention_tweets = get_tweets(ids['ZA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv files of kenya pre tweets\n",
    "names = [*range(len(tags))]\n",
    "names =  [str(x) for x in names]\n",
    "\n",
    "for tweets, n in zip(pre_bowl, names):\n",
    "    t.get_var('KE', tweets, n, period = 'pre_int')\n",
    "    \n",
    "# csv files of kenya during tweets\n",
    "names = [*range((len(tags)))]\n",
    "names =  [str(x) for x in names]\n",
    "\n",
    "for tweets, n in zip(during_bowl, names):\n",
    "    t.get_var('KE', tweets, n, period = 'during_int')\n",
    "    \n",
    "    \n",
    "# csv files of kenya post tweets\n",
    "names = [*range(len(tags))]\n",
    "names =  [str(x) for x in names]\n",
    "\n",
    "for tweets, n in zip(post_bowl, names):\n",
    "    t.get_var('KE', tweets, n, period = 'post_int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
